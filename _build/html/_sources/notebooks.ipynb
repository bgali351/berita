{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "link aplikasi\n",
    "https://huggingface.co/spaces/bgali/cek_kategori_berita"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# scrapping kompas.com\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## import library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup as bs\n",
    "import requests\n",
    "import pandas as pd\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## preprocessing text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_text(txt):\n",
    "  txt = txt.replace('Baca berita tanpa iklan.', '')\n",
    "  txt = txt.replace('\\xa0', '')\n",
    "  txt = txt.replace('\\n', '')\n",
    "  return txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## scrapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scraping(soup):\n",
    "    isi_berita = {}\n",
    "\n",
    "    title = soup.find('h1', {'class': 'read__title'})\n",
    "    isi_berita[\"judul\"] = title.text.strip() if title else \"Title not found\"\n",
    "\n",
    "    rplc = []\n",
    "    berita = soup.find('div', {'class': 'read__content'})\n",
    "    if berita:\n",
    "        for paragraph in berita.find_all('p'):\n",
    "            rplc.append(replace_text(paragraph.text))\n",
    "        isi_berita[\"isi\"] = \" \".join(rplc)\n",
    "    else:\n",
    "        isi_berita[\"isi\"] = \"Content not found\"\n",
    "\n",
    "    date = soup.find('div', {'class': 'read__time'})\n",
    "    isi_berita[\"tanggal\"] = date.text.strip() if date else \"Date not found\"\n",
    "\n",
    "    category = soup.select_one('body > div.wrap > div.container.clearfix > div:nth-child(4) > div > h3 > ul > li:nth-child(3) > a > span')\n",
    "    isi_berita[\"kategori\"] = category.text.strip() if category else \"Category not found\"\n",
    "    return isi_berita\n",
    "\n",
    "def linked_news(url):\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()\n",
    "        return bs(response.text, \"html5lib\")\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Request failed: {e}\")\n",
    "        return None\n",
    "    \n",
    "def link_berita(soup):\n",
    "    if soup:\n",
    "        konten = soup.find('div', {'class': 'articleList -list'})\n",
    "        if konten:\n",
    "            articles = konten.find_all(\"div\", class_=\"articleItem\")\n",
    "            print(f\"Found {len(articles)} articles on this page.\")  # Debugging print\n",
    "            return articles\n",
    "    print(\"No articles found.\")  # Debugging print\n",
    "    return []\n",
    "\n",
    "def start(page=1):\n",
    "    base_url = 'https://indeks.kompas.com/?site=news&page='\n",
    "    news = []\n",
    "\n",
    "    for current_page in range(1, page + 1):\n",
    "        url = base_url + str(current_page)\n",
    "        print(f\"Fetching page: {url}\")  # Debugging print\n",
    "        soup = linked_news(url)\n",
    "        if soup:\n",
    "            articles = link_berita(soup)\n",
    "            for article in articles:\n",
    "                new_link = article.find('a')['href']\n",
    "                soup_berita = linked_news(new_link)\n",
    "                if soup_berita:\n",
    "                    hasil = scraping(soup_berita)\n",
    "                    news.append(hasil)\n",
    "        time.sleep(1)\n",
    "    return news\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## masukkan jumlah halaman yang ingin di scrapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching page: https://indeks.kompas.com/?site=news&page=1\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'requests' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 25\u001b[0m, in \u001b[0;36mlinked_news\u001b[1;34m(url)\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 25\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mrequests\u001b[49m\u001b[38;5;241m.\u001b[39mget(url)\n\u001b[0;32m     26\u001b[0m     response\u001b[38;5;241m.\u001b[39mraise_for_status()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'requests' is not defined",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m page \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(\u001b[38;5;28minput\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMasukkan jumlah halaman: \u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[1;32m----> 2\u001b[0m berita \u001b[38;5;241m=\u001b[39m \u001b[43mstart\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m berita:\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;66;03m# Create a DataFrame and display it\u001b[39;00m\n\u001b[0;32m      6\u001b[0m     dataFrame \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(berita)\n",
      "Cell \u001b[1;32mIn[2], line 49\u001b[0m, in \u001b[0;36mstart\u001b[1;34m(page)\u001b[0m\n\u001b[0;32m     47\u001b[0m url \u001b[38;5;241m=\u001b[39m base_url \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(current_page)\n\u001b[0;32m     48\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFetching page: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00murl\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)  \u001b[38;5;66;03m# Debugging print\u001b[39;00m\n\u001b[1;32m---> 49\u001b[0m soup \u001b[38;5;241m=\u001b[39m \u001b[43mlinked_news\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     50\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m soup:\n\u001b[0;32m     51\u001b[0m     articles \u001b[38;5;241m=\u001b[39m link_berita(soup)\n",
      "Cell \u001b[1;32mIn[2], line 28\u001b[0m, in \u001b[0;36mlinked_news\u001b[1;34m(url)\u001b[0m\n\u001b[0;32m     26\u001b[0m     response\u001b[38;5;241m.\u001b[39mraise_for_status()\n\u001b[0;32m     27\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m bs(response\u001b[38;5;241m.\u001b[39mtext, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhtml5lib\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 28\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[43mrequests\u001b[49m\u001b[38;5;241m.\u001b[39mexceptions\u001b[38;5;241m.\u001b[39mRequestException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     29\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRequest failed: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     30\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'requests' is not defined"
     ]
    }
   ],
   "source": [
    "page = int(input('Masukkan jumlah halaman: '))\n",
    "berita = start(page)\n",
    "\n",
    "if berita:\n",
    "    # Create a DataFrame and display it\n",
    "    dataFrame = pd.DataFrame(berita)\n",
    "    print(dataFrame)\n",
    "\n",
    "    # Save DataFrame to a CSV file\n",
    "    csv_filename = 'berita_kompas.csv'\n",
    "    dataFrame.to_csv(csv_filename, index=False, encoding='utf-8-sig')\n",
    "    print(f\"Data has been saved to {csv_filename}\")\n",
    "    \n",
    "    # Save DataFrame to a Pickle file\n",
    "    pkl_filename = 'berita_kompas.pkl'\n",
    "    dataFrame.to_pickle(pkl_filename)\n",
    "    print(f\"Data has also been saved to {pkl_filename}\")\n",
    "else:\n",
    "    print(\"No news data was collected.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
