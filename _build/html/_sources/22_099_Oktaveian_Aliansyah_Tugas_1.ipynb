{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sxX55pozBQJy"
   },
   "source": [
    "# crawling berita di kompas\n",
    "\n",
    "## apa itu crawling?\n",
    "\n",
    "crawling merupakan program otomastis untuk mengumpulkan informasi dari berbagai halaman web. program crawling yang bisa di sebut juga \"web spider\" yang secara sistematis mengunjungi halaman-halaman web dan mengumpulkan data seperti teks, gambar dan metadata.\n",
    "\n",
    "proses crawling ini biasanya digunakan oleh mesin search engine untuk mengindeks konten web, sehingga ketika orang melakukan pencarian, mesin pencari dapat dengan cepat memberikan konten yang relevan."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "47tIjVVJEGO9"
   },
   "source": [
    "#proses melakukan crawling\n",
    "##tool atau library\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "0e5Dn0R4BHtI"
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup as bs\n",
    "import requests\n",
    "import pandas as pd\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RrWSeR7HFKX2"
   },
   "source": [
    "- BeautifulSoup berfungsi untuk parsing HTML dan ekstraksi data\n",
    "- request berfungsi untuk mengirim permintaan HTTP\n",
    "- pandas berfungsi untuk membaca dan menganalisis data yang disimpan\n",
    "- time berfungsi untuk memberikan delay agar server tidak overload"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xZYMkLe7INrs"
   },
   "source": [
    "#code program\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "rxJjAezmKfzR"
   },
   "outputs": [],
   "source": [
    "def replace_text(txt):\n",
    "  txt = txt.replace('Baca berita tanpa iklan.', '')\n",
    "  txt = txt.replace('\\xa0', '')\n",
    "  txt = txt.replace('\\n', '')\n",
    "  return txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ekPpr1EDNI1d"
   },
   "source": [
    "fungsi ini berfungsi untuk menghapus text yang tidak dibutuhkan\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "lgRsWHBKPPOR"
   },
   "outputs": [],
   "source": [
    "def scraping(soup):\n",
    "    isi_berita = {}\n",
    "\n",
    "    title = soup.find('h1', {'class': 'read__title'})\n",
    "    isi_berita[\"judul\"] = title.text.strip() if title else \"Title not found\"\n",
    "\n",
    "    rplc = []\n",
    "    berita = soup.find('div', {'class': 'read__content'})\n",
    "    if berita:\n",
    "        for paragraph in berita.find_all('p'):\n",
    "            rplc.append(replace_text(paragraph.text))\n",
    "        isi_berita[\"isi\"] = \" \".join(rplc)\n",
    "    else:\n",
    "        isi_berita[\"isi\"] = \"Content not found\"\n",
    "\n",
    "    date = soup.find('div', {'class': 'read__time'})\n",
    "    isi_berita[\"tanggal\"] = date.text.strip() if date else \"Date not found\"\n",
    "\n",
    "    category = soup.select_one('body > div.wrap > div.container.clearfix > div:nth-child(4) > div > h3 > ul > li:nth-child(3) > a > span')\n",
    "    isi_berita[\"kategori\"] = category.text.strip() if category else \"Category not found\"\n",
    "    return isi_berita\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2B_036D2XA_q"
   },
   "source": [
    "fungsi ini untuk proses scraping data dengan mengambil judul, pembersihan isi, tanggal berita, dan kategori berita"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "8_ZiRbtfIW7w"
   },
   "outputs": [],
   "source": [
    "def linked_news(url):\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()\n",
    "        return bs(response.text, \"html5lib\")\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Request failed: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DqrUJeLoKSx_"
   },
   "source": [
    "fungsi linked_news ini untuk mengambil isi html dalam web yang membutuhkan library request dan BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "ZmUysV2LZkI2"
   },
   "outputs": [],
   "source": [
    "def link_berita(soup):\n",
    "    if soup:\n",
    "        konten = soup.find('div', {'class': 'articleList -list'})\n",
    "        if konten:\n",
    "            articles = konten.find_all(\"div\", class_=\"articleItem\")\n",
    "            print(f\"Found {len(articles)} articles on this page.\")  # Debugging print\n",
    "            return articles\n",
    "    print(\"No articles found.\")  # Debugging print\n",
    "    return []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lrVx_FREeNHa"
   },
   "source": [
    "fungsi link_berita untuk mengambil semua berita yang berada di halaman hompage web"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "Jx7EDNXqeXHh"
   },
   "outputs": [],
   "source": [
    "def start(page=1):\n",
    "    base_url = 'https://indeks.kompas.com/?site=news&page='\n",
    "    news = []\n",
    "\n",
    "    for current_page in range(1, page + 1):\n",
    "        url = base_url + str(current_page)\n",
    "        print(f\"Fetching page: {url}\")  # Debugging print\n",
    "        soup = linked_news(url)\n",
    "        if soup:\n",
    "            articles = link_berita(soup)\n",
    "            for article in articles:\n",
    "                new_link = article.find('a')['href']\n",
    "                soup_berita = linked_news(new_link)\n",
    "                if soup_berita:\n",
    "                    hasil = scraping(soup_berita)\n",
    "                    news.append(hasil)\n",
    "        time.sleep(1)\n",
    "    return news"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UKoaSV5Oj6Ft"
   },
   "source": [
    "fungsi start ini untuk perulangan mengambil url berita pada halaman homepage, dan juga menggunakan funtion yang telah dibuat sebelumnya.\n",
    "\n",
    "kemudian ada program untuk memasukkan berapa halaman yang ingin di scraping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P8UKx39pkKHd"
   },
   "source": [
    "ini fungsi untuk memasukkannya ke dalam data frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching page: https://indeks.kompas.com/?site=news&page=1\n",
      "Found 15 articles on this page.\n",
      "Fetching page: https://indeks.kompas.com/?site=news&page=2\n",
      "Found 15 articles on this page.\n",
      "Fetching page: https://indeks.kompas.com/?site=news&page=3\n",
      "Found 15 articles on this page.\n",
      "Fetching page: https://indeks.kompas.com/?site=news&page=4\n",
      "Found 15 articles on this page.\n",
      "Fetching page: https://indeks.kompas.com/?site=news&page=5\n",
      "Found 15 articles on this page.\n",
      "Fetching page: https://indeks.kompas.com/?site=news&page=6\n",
      "Found 15 articles on this page.\n",
      "Fetching page: https://indeks.kompas.com/?site=news&page=7\n",
      "Found 15 articles on this page.\n",
      "Fetching page: https://indeks.kompas.com/?site=news&page=8\n",
      "Found 15 articles on this page.\n",
      "Fetching page: https://indeks.kompas.com/?site=news&page=9\n",
      "Found 15 articles on this page.\n",
      "Fetching page: https://indeks.kompas.com/?site=news&page=10\n",
      "Found 15 articles on this page.\n",
      "                                                 judul  \\\n",
      "0    Profil 5 Pimpinan KPK Periode 2024-2029, Didom...   \n",
      "1    Profil dan Harta Benny Mamoto, Dewas KPK Perio...   \n",
      "2    Bareskrim Polri Tangkap 1 Buron Kasus Judol Si...   \n",
      "3    Berpegang pada Prinsip GCG, KAI Pastikan Layan...   \n",
      "4    Ridwan Kamil Janji Buka Rincian APBD ke Publik...   \n",
      "..                                                 ...   \n",
      "145  APK-nya Banyak Dirusak dan Dicoret, Ridwan Kam...   \n",
      "146  Johanis Tanak Sebut Kesuksesan Penanganan Kasu...   \n",
      "147  Sekjen PDI-P: Seluruh Kader Wajib Menangkan Pa...   \n",
      "148   Dua Pelaku Begal di Kuburan Cina Depok Ditangkap   \n",
      "149  Mengapa Bawaslu Putuskan Prabowo Tak Langgar A...   \n",
      "\n",
      "                                                   isi  \\\n",
      "0    JAKARTA, KOMPAS.com - Komisi III Dewan Perwaki...   \n",
      "1    JAKARTA, KOMPAS.com - Komisi III DPR RI akhirn...   \n",
      "2    JAKARTA, KOMPAS.com - Direktorat Tindak Pidana...   \n",
      "3    KOMPAS.com - Vice President (VP) Public Relati...   \n",
      "4    JAKARTA, KOMPAS.com - Calon gubernur Jakarta n...   \n",
      "..                                                 ...   \n",
      "145  JAKARTA, KOMPAS.com – Calon gubernur Jakarta n...   \n",
      "146  JAKARTA, KOMPAS.com - Wakil Ketua KPK Johanis ...   \n",
      "147  JAKARTA, KOMPAS.com - Sekretaris Jenderal PDI-...   \n",
      "148  DEPOK, KOMPAS.com – Polisi berhasil menangkap ...   \n",
      "149  Badan Pengawas Pemilu (Bawaslu) RI memutuskan ...   \n",
      "\n",
      "                                tanggal     kategori  \n",
      "0    Kompas.com - 21/11/2024, 13:51 WIB     Nasional  \n",
      "1    Kompas.com - 21/11/2024, 13:50 WIB     Nasional  \n",
      "2    Kompas.com - 21/11/2024, 13:44 WIB     Nasional  \n",
      "3    Kompas.com - 21/11/2024, 13:44 WIB     Nasional  \n",
      "4    Kompas.com - 21/11/2024, 13:42 WIB  Megapolitan  \n",
      "..                                  ...          ...  \n",
      "145  Kompas.com - 20/11/2024, 17:55 WIB  Megapolitan  \n",
      "146  Kompas.com - 20/11/2024, 17:45 WIB     Nasional  \n",
      "147  Kompas.com - 20/11/2024, 17:45 WIB     Nasional  \n",
      "148  Kompas.com - 20/11/2024, 17:43 WIB  Megapolitan  \n",
      "149  Kompas.com - 20/11/2024, 17:42 WIB     Nasional  \n",
      "\n",
      "[150 rows x 4 columns]\n",
      "Data has been saved to berita_kompas.csv\n",
      "Data has also been saved to berita_kompas.pkl\n"
     ]
    }
   ],
   "source": [
    "page = int(input('Masukkan jumlah halaman: '))\n",
    "berita = start(page)\n",
    "\n",
    "if berita:\n",
    "    # Create a DataFrame and display it\n",
    "    dataFrame = pd.DataFrame(berita)\n",
    "    print(dataFrame)\n",
    "\n",
    "    # Save DataFrame to a CSV file\n",
    "    csv_filename = 'berita_kompas.csv'\n",
    "    dataFrame.to_csv(csv_filename, index=False, encoding='utf-8-sig')\n",
    "    print(f\"Data has been saved to {csv_filename}\")\n",
    "    \n",
    "    # Save DataFrame to a Pickle file\n",
    "    pkl_filename = 'berita_kompas.pkl'\n",
    "    dataFrame.to_pickle(pkl_filename)\n",
    "    print(f\"Data has also been saved to {pkl_filename}\")\n",
    "else:\n",
    "    print(\"No news data was collected.\")\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
