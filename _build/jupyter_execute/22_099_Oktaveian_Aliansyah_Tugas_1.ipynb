{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sxX55pozBQJy"
   },
   "source": [
    "# crawling berita di kompas\n",
    "\n",
    "## apa itu crawling?\n",
    "\n",
    "crawling merupakan program otomastis untuk mengumpulkan informasi dari berbagai halaman web. program crawling yang bisa di sebut juga \"web spider\" yang secara sistematis mengunjungi halaman-halaman web dan mengumpulkan data seperti teks, gambar dan metadata.\n",
    "\n",
    "proses crawling ini biasanya digunakan oleh mesin search engine untuk mengindeks konten web, sehingga ketika orang melakukan pencarian, mesin pencari dapat dengan cepat memberikan konten yang relevan."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "47tIjVVJEGO9"
   },
   "source": [
    "#proses melakukan crawling\n",
    "##tool atau library\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "0e5Dn0R4BHtI"
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup as bs\n",
    "import requests\n",
    "import pandas as pd\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RrWSeR7HFKX2"
   },
   "source": [
    "- BeautifulSoup berfungsi untuk parsing HTML dan ekstraksi data\n",
    "- request berfungsi untuk mengirim permintaan HTTP\n",
    "- pandas berfungsi untuk membaca dan menganalisis data yang disimpan\n",
    "- time berfungsi untuk memberikan delay agar server tidak overload"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xZYMkLe7INrs"
   },
   "source": [
    "#code program\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "rxJjAezmKfzR"
   },
   "outputs": [],
   "source": [
    "def replace_text(txt):\n",
    "  txt = txt.replace('Baca berita tanpa iklan.', '')\n",
    "  txt = txt.replace('\\xa0', '')\n",
    "  txt = txt.replace('\\n', '')\n",
    "  return txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ekPpr1EDNI1d"
   },
   "source": [
    "fungsi ini berfungsi untuk menghapus text yang tidak dibutuhkan\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "lgRsWHBKPPOR"
   },
   "outputs": [],
   "source": [
    "def scraping(soup):\n",
    "    isi_berita = {}\n",
    "\n",
    "    title = soup.find('h1', {'class': 'read__title'})\n",
    "    isi_berita[\"judul\"] = title.text.strip() if title else \"Title not found\"\n",
    "\n",
    "    rplc = []\n",
    "    berita = soup.find('div', {'class': 'read__content'})\n",
    "    if berita:\n",
    "        for paragraph in berita.find_all('p'):\n",
    "            rplc.append(replace_text(paragraph.text))\n",
    "        isi_berita[\"isi\"] = \" \".join(rplc)\n",
    "    else:\n",
    "        isi_berita[\"isi\"] = \"Content not found\"\n",
    "\n",
    "    date = soup.find('div', {'class': 'read__time'})\n",
    "    isi_berita[\"tanggal\"] = date.text.strip() if date else \"Date not found\"\n",
    "\n",
    "    category = soup.select_one('body > div.wrap > div.container.clearfix > div:nth-child(4) > div > h3 > ul > li:nth-child(3) > a > span')\n",
    "    isi_berita[\"kategori\"] = category.text.strip() if category else \"Category not found\"\n",
    "    return isi_berita\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2B_036D2XA_q"
   },
   "source": [
    "fungsi ini untuk proses scraping data dengan mengambil judul, pembersihan isi, tanggal berita, dan kategori berita"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "8_ZiRbtfIW7w"
   },
   "outputs": [],
   "source": [
    "def linked_news(url):\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()\n",
    "        return bs(response.text, \"html5lib\")\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Request failed: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DqrUJeLoKSx_"
   },
   "source": [
    "fungsi linked_news ini untuk mengambil isi html dalam web yang membutuhkan library request dan BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "ZmUysV2LZkI2"
   },
   "outputs": [],
   "source": [
    "def link_berita(soup):\n",
    "    if soup:\n",
    "        konten = soup.find('div', {'class': 'articleList -list'})\n",
    "        if konten:\n",
    "            articles = konten.find_all(\"div\", class_=\"articleItem\")\n",
    "            print(f\"Found {len(articles)} articles on this page.\")  # Debugging print\n",
    "            return articles\n",
    "    print(\"No articles found.\")  # Debugging print\n",
    "    return []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lrVx_FREeNHa"
   },
   "source": [
    "fungsi link_berita untuk mengambil semua berita yang berada di halaman hompage web"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "Jx7EDNXqeXHh"
   },
   "outputs": [],
   "source": [
    "def start(page=1):\n",
    "    base_url = 'https://indeks.kompas.com/?site=news&page='\n",
    "    news = []\n",
    "\n",
    "    for current_page in range(1, page + 1):\n",
    "        url = base_url + str(current_page)\n",
    "        print(f\"Fetching page: {url}\")  # Debugging print\n",
    "        soup = linked_news(url)\n",
    "        if soup:\n",
    "            articles = link_berita(soup)\n",
    "            for article in articles:\n",
    "                new_link = article.find('a')['href']\n",
    "                soup_berita = linked_news(new_link)\n",
    "                if soup_berita:\n",
    "                    hasil = scraping(soup_berita)\n",
    "                    news.append(hasil)\n",
    "        time.sleep(1)\n",
    "    return news"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UKoaSV5Oj6Ft"
   },
   "source": [
    "fungsi start ini untuk perulangan mengambil url berita pada halaman homepage, dan juga menggunakan funtion yang telah dibuat sebelumnya.\n",
    "\n",
    "kemudian ada program untuk memasukkan berapa halaman yang ingin di scraping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P8UKx39pkKHd"
   },
   "source": [
    "ini fungsi untuk memasukkannya ke dalam data frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "StdinNotImplementedError",
     "evalue": "raw_input was called, but this frontend does not support input requests.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mStdinNotImplementedError\u001b[0m                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m page \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mMasukkan jumlah halaman: \u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m)\n\u001b[0;32m      2\u001b[0m berita \u001b[38;5;241m=\u001b[39m start(page)\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m berita:\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;66;03m# Create a DataFrame and display it\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\ipykernel\\kernelbase.py:1281\u001b[0m, in \u001b[0;36mKernel.raw_input\u001b[1;34m(self, prompt)\u001b[0m\n\u001b[0;32m   1279\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_allow_stdin:\n\u001b[0;32m   1280\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraw_input was called, but this frontend does not support input requests.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 1281\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m StdinNotImplementedError(msg)\n\u001b[0;32m   1282\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_input_request(\n\u001b[0;32m   1283\u001b[0m     \u001b[38;5;28mstr\u001b[39m(prompt),\n\u001b[0;32m   1284\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_parent_ident[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshell\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m   1285\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_parent(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshell\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m   1286\u001b[0m     password\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m   1287\u001b[0m )\n",
      "\u001b[1;31mStdinNotImplementedError\u001b[0m: raw_input was called, but this frontend does not support input requests."
     ]
    }
   ],
   "source": [
    "page = int(input('Masukkan jumlah halaman: '))\n",
    "berita = start(page)\n",
    "\n",
    "if berita:\n",
    "    # Create a DataFrame and display it\n",
    "    dataFrame = pd.DataFrame(berita)\n",
    "    print(dataFrame)\n",
    "\n",
    "    # Save DataFrame to a CSV file\n",
    "    csv_filename = 'berita_kompas.csv'\n",
    "    dataFrame.to_csv(csv_filename, index=False, encoding='utf-8-sig')\n",
    "    print(f\"Data has been saved to {csv_filename}\")\n",
    "    \n",
    "    # Save DataFrame to a Pickle file\n",
    "    pkl_filename = 'berita_kompas.pkl'\n",
    "    dataFrame.to_pickle(pkl_filename)\n",
    "    print(f\"Data has also been saved to {pkl_filename}\")\n",
    "else:\n",
    "    print(\"No news data was collected.\")\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}