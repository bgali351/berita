{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "link aplikasi\n",
    "https://huggingface.co/spaces/bgali/cek_kategori_berita"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# scrapping kompas.com\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## import library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup as bs\n",
    "import requests\n",
    "import pandas as pd\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## preprocessing text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_text(txt):\n",
    "  txt = txt.replace('Baca berita tanpa iklan.', '')\n",
    "  txt = txt.replace('\\xa0', '')\n",
    "  txt = txt.replace('\\n', '')\n",
    "  return txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## scrapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scraping(soup):\n",
    "    isi_berita = {}\n",
    "\n",
    "    title = soup.find('h1', {'class': 'read__title'})\n",
    "    isi_berita[\"judul\"] = title.text.strip() if title else \"Title not found\"\n",
    "\n",
    "    rplc = []\n",
    "    berita = soup.find('div', {'class': 'read__content'})\n",
    "    if berita:\n",
    "        for paragraph in berita.find_all('p'):\n",
    "            rplc.append(replace_text(paragraph.text))\n",
    "        isi_berita[\"isi\"] = \" \".join(rplc)\n",
    "    else:\n",
    "        isi_berita[\"isi\"] = \"Content not found\"\n",
    "\n",
    "    date = soup.find('div', {'class': 'read__time'})\n",
    "    isi_berita[\"tanggal\"] = date.text.strip() if date else \"Date not found\"\n",
    "\n",
    "    category = soup.select_one('body > div.wrap > div.container.clearfix > div:nth-child(4) > div > h3 > ul > li:nth-child(3) > a > span')\n",
    "    isi_berita[\"kategori\"] = category.text.strip() if category else \"Category not found\"\n",
    "    return isi_berita\n",
    "\n",
    "def linked_news(url):\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()\n",
    "        return bs(response.text, \"html5lib\")\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Request failed: {e}\")\n",
    "        return None\n",
    "    \n",
    "def link_berita(soup):\n",
    "    if soup:\n",
    "        konten = soup.find('div', {'class': 'articleList -list'})\n",
    "        if konten:\n",
    "            articles = konten.find_all(\"div\", class_=\"articleItem\")\n",
    "            print(f\"Found {len(articles)} articles on this page.\")  # Debugging print\n",
    "            return articles\n",
    "    print(\"No articles found.\")  # Debugging print\n",
    "    return []\n",
    "\n",
    "def start(page=1):\n",
    "    base_url = 'https://indeks.kompas.com/?site=news&page='\n",
    "    news = []\n",
    "\n",
    "    for current_page in range(1, page + 1):\n",
    "        url = base_url + str(current_page)\n",
    "        print(f\"Fetching page: {url}\")  # Debugging print\n",
    "        soup = linked_news(url)\n",
    "        if soup:\n",
    "            articles = link_berita(soup)\n",
    "            for article in articles:\n",
    "                new_link = article.find('a')['href']\n",
    "                soup_berita = linked_news(new_link)\n",
    "                if soup_berita:\n",
    "                    hasil = scraping(soup_berita)\n",
    "                    news.append(hasil)\n",
    "        time.sleep(1)\n",
    "    return news\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## masukkan jumlah halaman yang ingin di scrapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "StdinNotImplementedError",
     "evalue": "raw_input was called, but this frontend does not support input requests.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mStdinNotImplementedError\u001b[0m                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m page \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mMasukkan jumlah halaman: \u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m)\n\u001b[0;32m      2\u001b[0m berita \u001b[38;5;241m=\u001b[39m start(page)\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m berita:\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;66;03m# Create a DataFrame and display it\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\ipykernel\\kernelbase.py:1281\u001b[0m, in \u001b[0;36mKernel.raw_input\u001b[1;34m(self, prompt)\u001b[0m\n\u001b[0;32m   1279\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_allow_stdin:\n\u001b[0;32m   1280\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraw_input was called, but this frontend does not support input requests.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 1281\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m StdinNotImplementedError(msg)\n\u001b[0;32m   1282\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_input_request(\n\u001b[0;32m   1283\u001b[0m     \u001b[38;5;28mstr\u001b[39m(prompt),\n\u001b[0;32m   1284\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_parent_ident[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshell\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m   1285\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_parent(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshell\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m   1286\u001b[0m     password\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m   1287\u001b[0m )\n",
      "\u001b[1;31mStdinNotImplementedError\u001b[0m: raw_input was called, but this frontend does not support input requests."
     ]
    }
   ],
   "source": [
    "page = int(input('Masukkan jumlah halaman: '))\n",
    "berita = start(page)\n",
    "\n",
    "if berita:\n",
    "    # Create a DataFrame and display it\n",
    "    dataFrame = pd.DataFrame(berita)\n",
    "    print(dataFrame)\n",
    "\n",
    "    # Save DataFrame to a CSV file\n",
    "    csv_filename = 'berita_kompas.csv'\n",
    "    dataFrame.to_csv(csv_filename, index=False, encoding='utf-8-sig')\n",
    "    print(f\"Data has been saved to {csv_filename}\")\n",
    "    \n",
    "    # Save DataFrame to a Pickle file\n",
    "    pkl_filename = 'berita_kompas.pkl'\n",
    "    dataFrame.to_pickle(pkl_filename)\n",
    "    print(f\"Data has also been saved to {pkl_filename}\")\n",
    "else:\n",
    "    print(\"No news data was collected.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}