{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sxX55pozBQJy"
   },
   "source": [
    "# crawling berita di kompas\n",
    "\n",
    "## apa itu crawling?\n",
    "\n",
    "crawling merupakan program otomastis untuk mengumpulkan informasi dari berbagai halaman web. program crawling yang bisa di sebut juga \"web spider\" yang secara sistematis mengunjungi halaman-halaman web dan mengumpulkan data seperti teks, gambar dan metadata.\n",
    "\n",
    "proses crawling ini biasanya digunakan oleh mesin search engine untuk mengindeks konten web, sehingga ketika orang melakukan pencarian, mesin pencari dapat dengan cepat memberikan konten yang relevan."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "47tIjVVJEGO9"
   },
   "source": [
    "#proses melakukan crawling\n",
    "##tool atau library\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "0e5Dn0R4BHtI"
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup as bs\n",
    "import requests\n",
    "import pandas as pd\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RrWSeR7HFKX2"
   },
   "source": [
    "- BeautifulSoup berfungsi untuk parsing HTML dan ekstraksi data\n",
    "- request berfungsi untuk mengirim permintaan HTTP\n",
    "- pandas berfungsi untuk membaca dan menganalisis data yang disimpan\n",
    "- time berfungsi untuk memberikan delay agar server tidak overload"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xZYMkLe7INrs"
   },
   "source": [
    "#code program\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "rxJjAezmKfzR"
   },
   "outputs": [],
   "source": [
    "def replace_text(txt):\n",
    "  txt = txt.replace('Baca berita tanpa iklan.', '')\n",
    "  txt = txt.replace('\\xa0', '')\n",
    "  txt = txt.replace('\\n', '')\n",
    "  return txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ekPpr1EDNI1d"
   },
   "source": [
    "fungsi ini berfungsi untuk menghapus text yang tidak dibutuhkan\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "lgRsWHBKPPOR"
   },
   "outputs": [],
   "source": [
    "def scraping(soup):\n",
    "    isi_berita = {}\n",
    "\n",
    "    title = soup.find('h1', {'class': 'read__title'})\n",
    "    isi_berita[\"judul\"] = title.text.strip() if title else \"Title not found\"\n",
    "\n",
    "    rplc = []\n",
    "    berita = soup.find('div', {'class': 'read__content'})\n",
    "    if berita:\n",
    "        for paragraph in berita.find_all('p'):\n",
    "            rplc.append(replace_text(paragraph.text))\n",
    "        isi_berita[\"isi\"] = \" \".join(rplc)\n",
    "    else:\n",
    "        isi_berita[\"isi\"] = \"Content not found\"\n",
    "\n",
    "    date = soup.find('div', {'class': 'read__time'})\n",
    "    isi_berita[\"tanggal\"] = date.text.strip() if date else \"Date not found\"\n",
    "\n",
    "    category = soup.select_one('body > div.wrap > div.container.clearfix > div:nth-child(4) > div > h3 > ul > li:nth-child(3) > a > span')\n",
    "    isi_berita[\"kategori\"] = category.text.strip() if category else \"Category not found\"\n",
    "    return isi_berita\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2B_036D2XA_q"
   },
   "source": [
    "fungsi ini untuk proses scraping data dengan mengambil judul, pembersihan isi, tanggal berita, dan kategori berita"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "8_ZiRbtfIW7w"
   },
   "outputs": [],
   "source": [
    "def linked_news(url):\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()\n",
    "        return bs(response.text, \"html5lib\")\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Request failed: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DqrUJeLoKSx_"
   },
   "source": [
    "fungsi linked_news ini untuk mengambil isi html dalam web yang membutuhkan library request dan BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "ZmUysV2LZkI2"
   },
   "outputs": [],
   "source": [
    "def link_berita(soup):\n",
    "    if soup:\n",
    "        konten = soup.find('div', {'class': 'articleList -list'})\n",
    "        if konten:\n",
    "            articles = konten.find_all(\"div\", class_=\"articleItem\")\n",
    "            print(f\"Found {len(articles)} articles on this page.\")  # Debugging print\n",
    "            return articles\n",
    "    print(\"No articles found.\")  # Debugging print\n",
    "    return []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lrVx_FREeNHa"
   },
   "source": [
    "fungsi link_berita untuk mengambil semua berita yang berada di halaman hompage web"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "Jx7EDNXqeXHh"
   },
   "outputs": [],
   "source": [
    "def start(page=1):\n",
    "    base_url = 'https://indeks.kompas.com/?site=news&page='\n",
    "    news = []\n",
    "\n",
    "    for current_page in range(1, page + 1):\n",
    "        url = base_url + str(current_page)\n",
    "        print(f\"Fetching page: {url}\")  # Debugging print\n",
    "        soup = linked_news(url)\n",
    "        if soup:\n",
    "            articles = link_berita(soup)\n",
    "            for article in articles:\n",
    "                new_link = article.find('a')['href']\n",
    "                soup_berita = linked_news(new_link)\n",
    "                if soup_berita:\n",
    "                    hasil = scraping(soup_berita)\n",
    "                    news.append(hasil)\n",
    "        time.sleep(1)\n",
    "    return news"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UKoaSV5Oj6Ft"
   },
   "source": [
    "fungsi start ini untuk perulangan mengambil url berita pada halaman homepage, dan juga menggunakan funtion yang telah dibuat sebelumnya.\n",
    "\n",
    "kemudian ada program untuk memasukkan berapa halaman yang ingin di scraping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P8UKx39pkKHd"
   },
   "source": [
    "ini fungsi untuk memasukkannya ke dalam data frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching page: https://indeks.kompas.com/?site=news&page=1\n",
      "Found 15 articles on this page.\n",
      "Fetching page: https://indeks.kompas.com/?site=news&page=2\n",
      "Found 15 articles on this page.\n",
      "Fetching page: https://indeks.kompas.com/?site=news&page=3\n",
      "Found 15 articles on this page.\n",
      "Fetching page: https://indeks.kompas.com/?site=news&page=4\n",
      "Found 15 articles on this page.\n",
      "Fetching page: https://indeks.kompas.com/?site=news&page=5\n",
      "Found 15 articles on this page.\n",
      "Fetching page: https://indeks.kompas.com/?site=news&page=6\n",
      "Found 15 articles on this page.\n",
      "Fetching page: https://indeks.kompas.com/?site=news&page=7\n",
      "Found 15 articles on this page.\n",
      "Fetching page: https://indeks.kompas.com/?site=news&page=8\n",
      "Found 15 articles on this page.\n",
      "Fetching page: https://indeks.kompas.com/?site=news&page=9\n",
      "Found 15 articles on this page.\n",
      "Fetching page: https://indeks.kompas.com/?site=news&page=10\n",
      "Found 15 articles on this page.\n",
      "                                                 judul  \\\n",
      "0    Bilang \"Rakyat Jelata\" Saat Tanggapi Miftah Hi...   \n",
      "1    Buron Interpol Asal China Raup Rp 284 dari Bis...   \n",
      "2    Kebakaran Kafe di Kemang, Pegawai Menyelamatka...   \n",
      "3    Jaksa Tuntut Helena Lim Bayar Uang Pengganti R...   \n",
      "4    Real Count Pilkada Tangsel: Benyamin-Pilar Men...   \n",
      "..                                                 ...   \n",
      "145  Rekapitulasi Pilkada Jakarta 2024: Pramono-Ran...   \n",
      "146  Profil Felicia Tissue, Mantan Kekasih Kaesang ...   \n",
      "147  \"Real Count\" Pilkada Jakarta: Pramono-Rano Ung...   \n",
      "148  Bertemu Menko Yusril, Dubes Arab Saudi Laporka...   \n",
      "149  Teror Misterius Berulang di Bekasi Berbuntut P...   \n",
      "\n",
      "                                                   isi  \\\n",
      "0    JAKARTA, KOMPAS.com - Juru Bicara (Jubir) Kant...   \n",
      "1    JAKARTA, KOMPAS.com - Buron Interpol asal Chin...   \n",
      "2    JAKARTA, KOMPAS.com - Sebuah kafe di Kemang, J...   \n",
      "3    JAKARTA, KOMPAS.com - Pemilik perusahaan money...   \n",
      "4    TANGERANG SELATAN, KOMPAS.com - Calon wali kot...   \n",
      "..                                                 ...   \n",
      "145  JAKARTA, KOMPAS.com - Pasangan calon (paslon) ...   \n",
      "146  JAKARTA, KOMPAS.com - Sosok Felicia Tissue kem...   \n",
      "147  JAKARTA, KOMPAS.com - Perolehan suara pasangan...   \n",
      "148  JAKARTA, KOMPAS.com - Duta Besar Arab Saudi un...   \n",
      "149  BEKASI, KOMPAS.com â€“ Teror terhadap seorang pr...   \n",
      "\n",
      "                                tanggal     kategori  \n",
      "0    Kompas.com - 05/12/2024, 19:34 WIB     Nasional  \n",
      "1    Kompas.com - 05/12/2024, 19:34 WIB     Nasional  \n",
      "2    Kompas.com - 05/12/2024, 19:29 WIB  Megapolitan  \n",
      "3    Kompas.com - 05/12/2024, 19:27 WIB     Nasional  \n",
      "4    Kompas.com - 05/12/2024, 19:22 WIB  Megapolitan  \n",
      "..                                  ...          ...  \n",
      "145  Kompas.com - 05/12/2024, 09:13 WIB  Megapolitan  \n",
      "146  Kompas.com - 05/12/2024, 09:12 WIB     Nasional  \n",
      "147  Kompas.com - 05/12/2024, 09:07 WIB  Megapolitan  \n",
      "148  Kompas.com - 05/12/2024, 08:54 WIB     Nasional  \n",
      "149  Kompas.com - 05/12/2024, 08:51 WIB  Megapolitan  \n",
      "\n",
      "[150 rows x 4 columns]\n",
      "Data has been saved to berita_kompas.csv\n",
      "Data has also been saved to berita_kompas.pkl\n"
     ]
    }
   ],
   "source": [
    "page = int(input('Masukkan jumlah halaman: '))\n",
    "berita = start(page)\n",
    "\n",
    "if berita:\n",
    "    # Create a DataFrame and display it\n",
    "    dataFrame = pd.DataFrame(berita)\n",
    "    print(dataFrame)\n",
    "\n",
    "    # Save DataFrame to a CSV file\n",
    "    csv_filename = 'berita_kompas.csv'\n",
    "    dataFrame.to_csv(csv_filename, index=False, encoding='utf-8-sig')\n",
    "    print(f\"Data has been saved to {csv_filename}\")\n",
    "    \n",
    "    # Save DataFrame to a Pickle file\n",
    "    pkl_filename = 'berita_kompas.pkl'\n",
    "    dataFrame.to_pickle(pkl_filename)\n",
    "    print(f\"Data has also been saved to {pkl_filename}\")\n",
    "else:\n",
    "    print(\"No news data was collected.\")\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
